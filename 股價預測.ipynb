{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "- 下載科技股歷史紀錄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Stock\n",
    "import requests\n",
    "import yfinance as yf\n",
    "'''link = 'https://quality.data.gov.tw/dq_download_json.php?nid=11549&md5_url=bb878d47ffbe7b83bfc1b41d0b24946e'\n",
    "all_stock = requests.get(link)\n",
    "all_stock = pd.DataFrame(all_stock.json())\n",
    "stockid_list = []\n",
    "for i in all_stock['證券代號']:\n",
    "    stockid = float(i[:4])\n",
    "    if ((stockid>=2300 and stockid<2500) or \n",
    "        (stockid>=3000 and stockid<3100) or stockid==6128):\n",
    "        stockid_list.append(i+\".TW\")'''\n",
    "\n",
    "stockid_list = [i for i in range(2300, 2501)]\n",
    "[stockid_list.append(i) for i in range(3000, 3101)]\n",
    "\n",
    "\n",
    "stocks = pd.DataFrame()\n",
    "stocks['證券代號'] = stockid_list\n",
    "# stocks.reindex\n",
    "stocks['證券歷史'] = None\n",
    "\n",
    "allstock_hist = []\n",
    "n = 0\n",
    "for k, i in enumerate(stockid_list):\n",
    "    try:\n",
    "        i = str(i) + '.TW'\n",
    "        hist = yf.download(i)\n",
    "        stocks['證券歷史'][k] = hist\n",
    "    except:\n",
    "        stocks['證券歷史'][k] = None\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/stock_allhistory', 'wb') as fi:\n",
    "    pickle.dump(stocks, fi)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def get_stockhistdata(filepath):\n",
    "    if os.path.getsize(filepath) > 0:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            unpicker = pickle.Unpickler(f)\n",
    "            scores = unpicker.load()\n",
    "    else:\n",
    "        return 'bad'\n",
    "    return scores\n",
    "\n",
    "filepath = 'Data/stock_history'\n",
    "hist_data = get_stockhistdata('Data/stock_history')\n",
    "hist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.getsize('data/stock_history') > 0:  \n",
    "  \n",
    "    with open('data/stock_history', \"rb\") as f:\n",
    "        unpickler = pickle.Unpickler(f)\n",
    "        # if file is not empty scores will be equal\n",
    "        # to the value unpickled\n",
    "        scores = unpickler.load()\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Target Stock and Categorical label\n",
    "- 標記買進賣出訊號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 預測20天後股價上漲或下跌\n",
    "days_lag = -5\n",
    "label = []\n",
    "for i in (hist_data['證券歷史'][1]['Close'].shift(days_lag)/\n",
    "    hist_data['證券歷史'][1]['Close']):\n",
    "    if i>1.00:\n",
    "        label.append(1)\n",
    "    elif i <1.00:\n",
    "        label.append(-1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "\n",
    "stock_categorical = pd.DataFrame(hist_data['證券歷史'][1])\n",
    "stock_categorical['label'] = label\n",
    "\n",
    "stock_categorical['Close'][5000:-6].plot()\n",
    "stock_categorical['label'][5000:].plot(secondary_y = True)\n",
    "stock_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler((0, 1))\n",
    "scaler.fit_transform(stock_categorical, y=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = 2\n",
    "data_X = []\n",
    "data_y = []\n",
    "# 去掉最後days_lag的比數，因為沒資料加入0\n",
    "for i in range(win, (len(stock_categorical)+days_lag)):\n",
    "    # print(stock_categorical.iloc[i:i+win, :6])\n",
    "    data_X.append(stock_categorical.iloc[i-win:i, :])\n",
    "    data_y.append(stock_categorical.iloc[i, 6])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "- Test 不測試準確度，用在回測部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4532,)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = np.array(data_X[:int(len(data_X)*0.8)])\n",
    "train_y = np.array(data_y[:int(len(data_y)*0.8)])\n",
    "test_X = np.array(data_X[int(len(data_X)*0.8):])\n",
    "test_y = np.array(data_y[int(len(data_y)*0.8):])\n",
    "train_y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding: Onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        X_, y_ = self.X[index], self.y[index]\n",
    "        return X_, y_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "train_X = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float32)\n",
    "onehotencoder = OneHotEncoder()\n",
    "train_yencoding = onehotencoder.fit_transform(train_y.reshape(len(train_y), 1)).toarray()\n",
    "\n",
    "test_X = torch.tensor(test_X, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(train_X, train_yencoding), batch_size = 300, shuffle=False, drop_last = True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(test_X, test_y), batch_size = 300, shuffle=False, drop_last = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding: Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        X_, y_ = self.X[index], self.y[index]\n",
    "        return X_, y_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "# from sklearn import preprocessing\n",
    "train_X = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float32)\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(train_y)\n",
    "\n",
    "test_X = torch.tensor(test_X, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(train_X, train_y), batch_size = 300, shuffle=False, drop_last = True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(test_X, test_y), batch_size = 300, shuffle=False, drop_last = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim# Hidden dimensions\n",
    "        self.num_layers = num_layers# Number of hidden layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)# (batch_dim, seq_dim, feature_dim)\n",
    "       \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)# Readout layer\n",
    "\n",
    "    def forward(self, i):\n",
    "        \n",
    "        h0 = torch.zeros([self.num_layers, i.shape[0], self.hidden_dim], dtype=torch.double) #.requires_grad_()\n",
    "        c0 = torch.zeros([self.num_layers, i.shape[0], self.hidden_dim], dtype=torch.double) #.requires_grad_()\n",
    "        \n",
    "        out, _ = self.lstm.forward(i, (h0.detach().float(), c0.detach().float()))  # output shape (batch, sequence, hidden_dim)\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        out_softmax = F.softmax(out)\n",
    "        # print(out_softmax)\n",
    "        # print('---------------')\n",
    "        # print(out_softmax)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Onehot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=7, hidden_dim=10, num_layers=2, output_dim=2)\n",
    "model.train()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr = 5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    all = 0\n",
    "    for _, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # print(y)\n",
    "        pred_y = model(X)\n",
    "        #print(pred_y.shape)\n",
    "        # print(pred_y[1])\n",
    "        #print(pred_y[1]*(-1))\n",
    "        loss = criterion(y, pred_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'-----{epoch}------')\n",
    "        for i in range(len(pred_y)):\n",
    "            if np.argmax(pred_y[i].detach().numpy()) == np.argmax(y[i].detach().numpy()):           \n",
    "                correct+=1\n",
    "            all+=1\n",
    "        print(correct/all)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=7, hidden_dim=10, num_layers=2, output_dim=1)\n",
    "model.train()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr = .05)\n",
    "criterion = torch.nn.MSELoss()\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    all = 0\n",
    "    for _, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # print(y)\n",
    "        pred_y = model(X)\n",
    "        \n",
    "        loss = criterion(pred_y, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        #print(pred_y)\n",
    "        print(f'-----{epoch}------')\n",
    "        for k, i in enumerate(pred_y):\n",
    "            if np.argmax(i.detach().numpy())-1 == y[k]:\n",
    "                print(y)\n",
    "                correct +=1\n",
    "            all += 1\n",
    "        print(correct/all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df95319d8ce4e1d89f5365ae10992bc1f65da593082b1d264e8f529830ec2f02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
